{
 "metadata": {
  "name": "",
  "signature": "sha256:1e841d7cc5111f35f7a694520f162f397399d048963fbe084652c3d3096c8cee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import mxnet as mx\n",
      "from mxnet import nd, autograd\n",
      "mx.random.seed(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Attaching gradients\n",
      "As a toy example, Let's say that we are interested in differentiating a function $ f = w * (x * *2) $ with respect to parameter x. We can start by assigning an initial value of x. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = nd.array([[1,2], [3,4]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we compute the gradient of f with respect to x, we will need a place to store it. In MXNet, we can tell an NDArray that we plan to store a gradient by invoking its attach_grad() method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x.attach_grad()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're going to define the function f and MXNet will generate a computation graph on the fly. It's as if MXNet turned on a recording device and captured the exact path by which each variable was generated.\n",
      "\n",
      "\n",
      "Note that building the computation graph requires a nontrivial amount of computation. So MXNet will only build the graph when explicitly told to do so. We can instruct MXNet to start recording by placing code inside a with autograd.record(): block."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with autograd.record():\n",
      "    y = x * 2\n",
      "    z = y * x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's backprop by calling z.backward(). When z has more than one entry, z.backward() is equivalent to mx.nd.sum(z).backward()."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.backward()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's see if this is the expected output. Remeber that y = x * 2, and z = x * y, so z should be equal to 2 * x * x. After, doing backprop with z.backward(), we expect to get back gradient dz/dx as follows: dy/dx = 2, dz/dx = 4 * x. So, if everything went according to plan, x.grad should consist of an NDArray with the values [[4, 8], [12, 16]]."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## x.grad has values because we use x.attach_grad() to store the gradients."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(x.grad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[  4.   8.]\n",
        " [ 12.  16.]]\n",
        "<NDArray 2x2 @cpu(0)>\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## y does not have a grad value"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(y.grad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Head gradients and the chain rule"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes when we call the backward method on an NDArray, e.g. y.backward(), where y is a function of x we are just interested in the derivative of y with respect to x. Mathematicians writet this as $\\dfrac{dy(x)}{dx}$. At other times, we may be interested in the gradient of z with respect to x, where z is a function of y, which in turn, is a function of x. That is, we are interested in $\\dfrac{d}{dx}z(y(x))$. Recall that by the chain rule $\\dfrac{d}{dx}z(y(x)) = \\dfrac{dz(y)}{dy}\\dfrac{dy(x)}{dx}$. So when y is part of larger functoin z, and we want x.grad to store $\\dfrac{dz}{dx}$, we can pass in the head gradient $\\dfrac{dz}{dy}$ as an input to backward(). The default argument is nd.ones_like(y)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The meaning of pass head gradient is when z changes some value say 10, then by what amount x is changed. In this case $\\Delta x$ is 40."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with autograd.record():\n",
      "    y = x * 2\n",
      "    z = y * x\n",
      "\n",
      "head_gradient = nd.array([[10, 1.], [.1, .01]])\n",
      "z.backward(head_gradient)\n",
      "print(x.grad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 40.           8.        ]\n",
        " [  1.20000005   0.16      ]]\n",
        "<NDArray 2x2 @cpu(0)>\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}